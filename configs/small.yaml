model:
  vocab_size: 32000
  n_layer: 12
  n_head: 12
  n_embd: 768
  n_positions: 2048
  rotary_pct: 1.0
  dropout: 0.0

data:
  block_size: 1024
  num_workers: 4
  shuffle: true
  seed: 42

training:
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 3e-4
  weight_decay: 0.1
  warmup_steps: 2000
  lr_scheduler_type: cosine
  max_steps: 100000
  logging_steps: 50
  save_steps: 2000
  eval_steps: 2000
  bf16: true
  gradient_checkpointing: true
  max_grad_norm: 1.0

paths:
  tokenizer_dir: artifacts/tokenizer
  output_dir: artifacts/checkpoints
